{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Required Libraries"
      ],
      "metadata": {
        "id": "Me3Pg-0P7w9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we will be importing several libraries:\n",
        "\n",
        "\n",
        "1.   Tensorflow - A commonly used library in artificial intelligence and machine learning. Includes several classes and functions that are crucial in building models.\n",
        "2.   Keras - A library that provides an interface for Tensorflow. Simplifies building neural networks since it is built in Python.\n",
        "3.   Numpy - A library that supports computational operations related to linear algebra, matrices, and fourier transform.\n",
        "6.   os - A library which allows us to interact with the operating system (files, directories, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "VSujUVF48Oq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX6EtllMhYRN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c310be98-114a-4f9c-a2a4-d0393cbd996b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "#test function to make sure our gpu is working\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking a Look at the Data"
      ],
      "metadata": {
        "id": "CP3P-ZIo_HWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training data is currently stored in a text file so we begin by opening and reading the file.\n",
        "\n",
        "To make sure our data is read properly, we print out a small chunk of the file."
      ],
      "metadata": {
        "id": "iZeIh9jCAMtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"/content/text.txt\", \"r\").read()\n",
        "print(text[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVx1vT0ai2iC",
        "outputId": "e1e6045f-2dfe-4ff0-fd9e-71ddce0d6c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to train the model to predict how Shakespeare puts together words so we'll also need to know how many different characters there are first (65 in this case)."
      ],
      "metadata": {
        "id": "6FbER6OMApiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(\"Number of Unique Characters: {}\".format(len(vocab)))"
      ],
      "metadata": {
        "id": "C3D7tt9-jKoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9acbb9-b299-4984-8e3b-5fea30260cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Unique Characters: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing the Text"
      ],
      "metadata": {
        "id": "sc6wOLriDTxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding and Decoding the characters**\n",
        "\n",
        "Before we even begin training the model, we need to process our data into something that the model can actually read and understand. In this case, we will be converting each character in the file into a numerical representation."
      ],
      "metadata": {
        "id": "3G1cxNn7KCn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping Each Char to an Index**\n",
        "\n",
        "We'll assign a number to each character in the vocabulary set using enumerate() and place the pairs in a dictionary. After we've done this, we'll go ahead and convert the entire text file into numbers and store it into a numpy array.\n",
        "\n",
        "**Mapping Each Index to a Char**\n",
        "\n",
        "Next, we'll store the entire list of characters as a numpy array as well so that we'll be able to use each character as a key in the dictionary when decoding.\n"
      ],
      "metadata": {
        "id": "KfHGl0X5BURW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2Index = {c : i for i, c in enumerate(vocab)}\n",
        "int_text = np.array([char2Index[i] for i in text])\n",
        "\n",
        "index2Char = np.array(vocab)"
      ],
      "metadata": {
        "id": "CGhXfejQm1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check that our conversions worked properly, we will print the dictionary and chunks of each new array to compare to each other.\n",
        "\n",
        "We use repr() to allow us to print out representations of each character so that it is easier to visualize."
      ],
      "metadata": {
        "id": "vogiIcUtDMM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Character to Index: \\n\")\n",
        "for i in char2Index:\n",
        "  print(\"  {:4s}: {:3d}\".format(repr(i), char2Index[i]))\n",
        "\n",
        "print(\"\\nInput text to Integer: \\n\")\n",
        "print('{} mapped to {}'.format(repr(text[:20]), int_text[:20]))"
      ],
      "metadata": {
        "id": "X19P-c-HNcOK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce65fb3-c84b-450e-e29a-b619385aa52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character to Index: \n",
            "\n",
            "  '\\n':   0\n",
            "  ' ' :   1\n",
            "  '!' :   2\n",
            "  '$' :   3\n",
            "  '&' :   4\n",
            "  \"'\" :   5\n",
            "  ',' :   6\n",
            "  '-' :   7\n",
            "  '.' :   8\n",
            "  '3' :   9\n",
            "  ':' :  10\n",
            "  ';' :  11\n",
            "  '?' :  12\n",
            "  'A' :  13\n",
            "  'B' :  14\n",
            "  'C' :  15\n",
            "  'D' :  16\n",
            "  'E' :  17\n",
            "  'F' :  18\n",
            "  'G' :  19\n",
            "  'H' :  20\n",
            "  'I' :  21\n",
            "  'J' :  22\n",
            "  'K' :  23\n",
            "  'L' :  24\n",
            "  'M' :  25\n",
            "  'N' :  26\n",
            "  'O' :  27\n",
            "  'P' :  28\n",
            "  'Q' :  29\n",
            "  'R' :  30\n",
            "  'S' :  31\n",
            "  'T' :  32\n",
            "  'U' :  33\n",
            "  'V' :  34\n",
            "  'W' :  35\n",
            "  'X' :  36\n",
            "  'Y' :  37\n",
            "  'Z' :  38\n",
            "  'a' :  39\n",
            "  'b' :  40\n",
            "  'c' :  41\n",
            "  'd' :  42\n",
            "  'e' :  43\n",
            "  'f' :  44\n",
            "  'g' :  45\n",
            "  'h' :  46\n",
            "  'i' :  47\n",
            "  'j' :  48\n",
            "  'k' :  49\n",
            "  'l' :  50\n",
            "  'm' :  51\n",
            "  'n' :  52\n",
            "  'o' :  53\n",
            "  'p' :  54\n",
            "  'q' :  55\n",
            "  'r' :  56\n",
            "  's' :  57\n",
            "  't' :  58\n",
            "  'u' :  59\n",
            "  'v' :  60\n",
            "  'w' :  61\n",
            "  'x' :  62\n",
            "  'y' :  63\n",
            "  'z' :  64\n",
            "\n",
            "Input text to Integer: \n",
            "\n",
            "'First Citizen:\\nBefor' mapped to [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data"
      ],
      "metadata": {
        "id": "UrjYp9_UGsTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step for us is to now split our data into sequences. Our model is going to be fed these sequences and it will, over time, learn to predict the characters that follow each sequence. This will allow it to generate text with Shakespearean mannerisms.\n",
        "\n",
        "We'll begin by setting the length of each sequence. We will set it to 150 characters here. Then, by using from_tensor_slices(), we can convert the text from vector form into a character index stream which we can split up into batches."
      ],
      "metadata": {
        "id": "SvdswdVgJwQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 150  # max number of characters that can be fed per input\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(int_text)"
      ],
      "metadata": {
        "id": "WnJtod57OqU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the .batch() function and setting the first parameter to the sequence length, we will be splitting the dataset into batches each with 150 consecutive characters. The drop_remainder parameter is set to True to ensure that each batch has the same outer dimension."
      ],
      "metadata": {
        "id": "SbyFQNbNKWbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "ny-j5qIqOZ-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we'll want to create tuples of these sequences that can actually be fed to our model. The reason why we need tuples is because for RNN models to work, there needs to be an input text and target text for it to output. The input text is the sequence itself and the target text is the sequence minus its first character but with an additional predicted character at the end.\n",
        "\n",
        "By calling map(create_input_target_pair) we will be applying the function to each element of our dataset."
      ],
      "metadata": {
        "id": "gxr6jF07Mj8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_input_target_pair(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(create_input_target_pair)"
      ],
      "metadata": {
        "id": "Ffi5St6dPAEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will shuffle the data before packing it into the final batches which will be fed into the model for training. Shuffling the data set allows for the training set to be more representative of the entire text that we have since it minimizes variance."
      ],
      "metadata": {
        "id": "r-QfGi0yMdeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer used for shuffling dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "ztgCa4EmOzPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c473a1af-a107-4585-9682-c6c865c0d465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_BatchDataset element_spec=(TensorSpec(shape=(64, 150), dtype=tf.int64, name=None), TensorSpec(shape=(64, 150), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Model"
      ],
      "metadata": {
        "id": "uN1I7aXTMv81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin building the model, we first set parameters that we will be feeding into the layers of the model."
      ],
      "metadata": {
        "id": "EtZt0jErMzG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "# Embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units= 1024"
      ],
      "metadata": {
        "id": "b0ZWBuYGPB0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model will be able to take in 64 sequences per batch as of now, however, we will want to be able to input single sequences after training the model so we'll need to be able to use a different batch size later. To do this, we will create a function that can reproduce the model with different batch sizes.\n",
        "\n",
        "We will be using what's called a character-level recurrent neural network for this workshop. Recurrent neural networks are generally used for predictive operations on sequential data. It works by incorporating memory to save information from prior inputs and previously seen elements as \"states\" to influence the current output. The reason why our RNN is classified as character-level is because it reads words as a series of characters.\n",
        "The model itself will consist of 3 layers:\n",
        "1. Input Layer - This layer uses an embedding layer to map each character-ID to a 256 dimension vector.\n",
        "2. GRU Layer - This layer is a type of recurrent neural network with a size of 1024 gradient descent units.\n",
        "3. Dense Layer - This is the output layer which is the same size as the size of vocab. It essentially outputs the probability of each character being the next one in the sequence."
      ],
      "metadata": {
        "id": "qxFMxaskNJuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QV2DdnP-PI5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "k9IBadiKPAlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "GEoryA_keAnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c11815f-9728-4111-9d30-8fc7bbd88a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (64, None, 1024)          3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4021569 (15.34 MB)\n",
            "Trainable params: 4021569 (15.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model, we will once again be using the adam learning algorithm and the sparse categorical crossenttropy loss function. We are setting the from_logits parameter to True to inform the loss function that the output values generated by the model are not normalized."
      ],
      "metadata": {
        "id": "nMgPFCrb-fDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "kV7T_Ry9Pkjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be able to load our weights and to save our training performance, we will need to configure a checkpoint directory. We'll be using the saved checkpoint when we reproduce the model with a different batch size."
      ],
      "metadata": {
        "id": "EvfyfTwF-1Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir_checkpoints = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(dir_checkpoints, \"checkpoint_{epoch}\")\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "metadata": {
        "id": "3noJ4ZR-PnRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the model, the last thing we need to do is set the number of epochs. In this specific workshop, more epochs will lead to less loss, however, in the interest of time, we will only be using 10 epochs. Feel free to modify this number and see how your model differs later on though."
      ],
      "metadata": {
        "id": "79b1SUCZ-CQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=10"
      ],
      "metadata": {
        "id": "g8V_6bAjPpnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "IZciyqsZPtf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b8981d-cb47-408f-ad0a-043220ef8e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "115/115 [==============================] - 19s 84ms/step - loss: 2.9145\n",
            "Epoch 2/10\n",
            "115/115 [==============================] - 11s 83ms/step - loss: 2.1567\n",
            "Epoch 3/10\n",
            "115/115 [==============================] - 11s 83ms/step - loss: 1.8870\n",
            "Epoch 4/10\n",
            "115/115 [==============================] - 11s 85ms/step - loss: 1.6989\n",
            "Epoch 5/10\n",
            "115/115 [==============================] - 12s 89ms/step - loss: 1.5747\n",
            "Epoch 6/10\n",
            "115/115 [==============================] - 12s 93ms/step - loss: 1.4877\n",
            "Epoch 7/10\n",
            "115/115 [==============================] - 13s 96ms/step - loss: 1.4265\n",
            "Epoch 8/10\n",
            "115/115 [==============================] - 13s 96ms/step - loss: 1.3777\n",
            "Epoch 9/10\n",
            "115/115 [==============================] - 12s 93ms/step - loss: 1.3384\n",
            "Epoch 10/10\n",
            "115/115 [==============================] - 12s 87ms/step - loss: 1.3047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we can view the location of our latest checkpoint."
      ],
      "metadata": {
        "id": "_UAcI1EH0Ad6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(dir_checkpoints)"
      ],
      "metadata": {
        "id": "qhcHsA7qWmga",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b6c0cfde-fa05-468a-852a-daf706a8bb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/checkpoint_10'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll go ahead and create a new model using the earlier function with a different batch size and weights which are loaded from our checkpoint."
      ],
      "metadata": {
        "id": "VC9wQJsV0LIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(dir_checkpoints))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "kL_BFHvSWnHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "56G9nNfC0cJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bff963f-4a14-418b-a8c4-6736d73ee328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 65)             66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4021569 (15.34 MB)\n",
            "Trainable params: 4021569 (15.34 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model is now ready to make predictions, and all we need is a custom function to prepare our input for the model. We have to do the following:\n",
        "\n",
        "1. Set the number of characters to generate\n",
        "2. Vectorizing the input (from string to numbers) to feed into the model\n",
        "3. Create an empty variable to store the result\n",
        "4. Create a temperature value to manually adjust variability of the predictions\n",
        "5. Feed the output to the model again for the next prediction\n",
        "6. Join the generated character to the final string"
      ],
      "metadata": {
        "id": "FNQ6qsYv2JW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "    num_generate = 1000 #Number of characters to be generated\n",
        "\n",
        "    input_eval = [char2Index[s] for s in start_string] # Converting input to indexes\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 0.5\n",
        "\n",
        "    # Reset any hidden states in the model\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Use a categorical distribution to get the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We set the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index2Char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "JKmkIiJCWqPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = input(\"Enter your starting string: \")\n",
        "print(generate_text(model, start_string=test))"
      ],
      "metadata": {
        "id": "WKhuGtiAWz-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d525df-1e1f-4cb5-bd72-6be056c54cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your starting string: oh thow is deth?\n",
            "oh thow is deth?\n",
            "\n",
            "GLOUCESTER:\n",
            "And so, sir, I know the cause of worthing doth\n",
            "Be a senden for a man could not stame a little good\n",
            "Show'd with the citizens, and his friends are no sounding to him and his words,\n",
            "That should he can crave the grave. The land and thy head,\n",
            "And sure the trumpets of Rome and her for him,\n",
            "And thou the very weeds of the thrown the heavens and my heart\n",
            "And stand up thy love, or my heart to them,\n",
            "When they are none of the court of thy word.\n",
            "\n",
            "KING RICHARD III:\n",
            "That should consure the causes of men are well prove\n",
            "And see him in a life being so did be cut one and rebuty.\n",
            "\n",
            "ROMEO:\n",
            "I am a godden for this face to come to make\n",
            "A flier and a very grave be the city is dead.\n",
            "\n",
            "PETRUCHIO:\n",
            "Ay, but that thou shalt not stand upon a charge\n",
            "Our such a cousin Claudio's death,\n",
            "And threaten and consiners, when he did such a cruel the rettle of him.\n",
            "\n",
            "PAULINA:\n",
            "No more of the courtier of the mother reason\n",
            "That should stand before the sun that he hath weary of the walls,\n",
            "But in my lady is dead on him.\n",
            "\n",
            "K\n"
          ]
        }
      ]
    }
  ]
}