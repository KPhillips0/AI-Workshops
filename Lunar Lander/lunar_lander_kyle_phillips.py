# -*- coding: utf-8 -*-
"""Lunar Lander - Kyle Phillips

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zSfoO4TmkGLQHrNYtO2gYlx1iNMcYkxv
"""

!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1
!pip3 install -U colabgymrender
!pip3 install imageio==2.5
!pip install pygame

!pip install gym[box2d]

import gym
import numpy as np
import matplotlib.pyplot as plt
from colabgymrender.recorder import Recorder
import os
import pygame
os.environ['SDL_VIDEODRIVER']='dummy'
pygame.display.set_mode((640,480))

env = gym.make("LunarLander-v2")
env = Recorder(env, './video') # specific to colab
next_state = env.reset()

done = False
while not done:
    action = np.random.randint(4)
    next_state, reward, done, _ = env.step(action)
env.play() # specific to colab
# env.close()

LEARNING_RATE = 0.15
DISCOUNT = 0.95
EPISODES = 500
BUCKET_SIZE = np.array([12, 12, 12, 12, 12, 12, 2, 2])

TOTAL_EPISODES = 0 # do not change this
env = gym.make("LunarLander-v2")

# get observation space and action space info
low = env.observation_space.low
high = env.observation_space.high
actions = env.action_space.n

# create bucket increments and intialize q-table
discrete_inc = (high - low) / BUCKET_SIZE
q_table = np.random.uniform(low=-8, high=5, size=(np.prod(BUCKET_SIZE), actions))

# create encoding vector
en_vec = []
for i in range(1, len(BUCKET_SIZE)):
    en_vec.append(np.prod(BUCKET_SIZE[i:len(BUCKET_SIZE)]))
en_vec.append(1)
en_vec = np.array(en_vec)

# map our continuous observation space array to a single index in our q-table (aka discrete state)
def get_discrete_state(state):
    discrete_state = np.minimum(BUCKET_SIZE-1, (state - low)/discrete_inc)
    return discrete_state.astype(np.int).dot(en_vec)

# perform a number of iterations/episodes for our model to train on
for episode in range(EPISODES):
    # reset position of lander and get state
    discrete_state = get_discrete_state(env.reset()[0])
    done = False
    if episode % 5 == 0:
        print("episode: %d" % episode)
    # main while loop to run the "game"
    while not done:
        # depending on where we are and more (aka our state), get the action
        # that would give us the highest q-value from or q-table
        action = np.argmax(q_table[discrete_state])
        next_state, reward, done, _ = env.step(action)
        next_discrete_state = get_discrete_state(next_state)
        if not done:
            # predict the highest q-value we can achieve with our current parameters
            max_future_q = np.max(q_table[next_discrete_state])
            current_q = q_table[discrete_state][action]
            # update our q-value
            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)
            q_table[discrete_state][action] = new_q
        # maybe not do
        elif False in (abs(next_state) <= np.array([1, 0.5, 1, 0.2, 1, 1, 1, 1])):
            q_table[discrete_state][action] += 100
        discrete_state = next_discrete_state
env.close()
TOTAL_EPISODES += EPISODES

# "testing mode" of our model
# parameters are not updated
test_env = gym.make("LunarLander-v2")
test_env = Recorder(test_env, './video') # specific to colab
test_env.reset()
print("trained on: %d episodes" % TOTAL_EPISODES)
done = False
while not done:
    action = np.argmax(q_table[discrete_state])
    next_state, reward, done, _ = test_env.step(action)
    next_discrete_state = get_discrete_state(next_state)
    discrete_state = next_discrete_state
test_env.play() # specific to colab
test_env.close()

np.save("q-table", q_table)

f = open('q-table.npy', 'rb')
q_table = np.load(f)
for episode in range(EPISODES):
    discrete_state = get_discrete_state(env.reset()[0])
    done = False
    if episode % 5 == 0:
        print("episode: %d" % episode)
    while not done:
        action = np.argmax(q_table[discrete_state])
        next_state, reward, done, _ = env.step(action)
        next_discrete_state = get_discrete_state(next_state)
        if not done:
            max_future_q = np.max(q_table[next_discrete_state])
            current_q = q_table[discrete_state][action]
            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)
            q_table[discrete_state][action] = new_q
        discrete_state = next_discrete_state
env.close()
f.close()
TOTAL_EPISODES += EPISODES