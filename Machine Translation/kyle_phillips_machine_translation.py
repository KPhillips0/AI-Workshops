# -*- coding: utf-8 -*-
"""Kyle Phillips - Machine Translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1OaHqJo4MSKMpSpE4UFs1LR265Rqb4l

# Importing the Necessary Libraries

We have many libraries to import for this workshop, though most functions we will be using come from keras.

1. string - This module contains some constants, utility function, and classes for string manipulation.
2. pandas - This library is focused on data manipulation and analysis.
3. NumPy - This library is used for working with arrays and working in the domain of linear algebra, fourier transform, and matrices.
4. Keras - Keras provides an interface for tensorflow in Python.
5. scikit-learn - This library contains a lot of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.
6. Matplotlib - A cross-platform, data visualization and graphical plotting library for Python and its numerical extension NumPy.
"""

import string
from numpy import array, argmax
import pandas as pd
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, RepeatVector
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from keras import optimizers
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
pd.set_option('display.max_colwidth', 200)

"""# Preparing the Data

We'll start by opening our file and reading in all the text. Our data is currently a text file which contains english words and phrases and the Italian translations of each.
"""

def read_text(filename):
        # open the file
        file = open(filename, mode='rt', encoding='utf-8')

        # read all text
        text = file.read()
        file.close()
        return text

"""Because the English and Italian phrases are paired together in the text file, we need to separate them first. We'll use the split() function to first separate the file by each row and then use the strip() function to get rid of the spaces at the front and back of each row. Next, we'll split each row into English phrases and their Italian counterpart by using the split() function on tabs which separate the two in the text. Lastly, we need to remove the extra information that is at the end of each row by simply deleting any string that includes CC-BY."""

def to_lines(text):
      sents = text.strip().split('\n')
      sents = [i.split('\t') for i in sents]
      for i in range(len(sents)):
        for j in sents[i]:
          if "CC-BY" in j:
            sents[i].remove(j)
      return sents

"""Here, we'll run the functions we wrote and then finally, shorten the data to only the first 50,000 row entries because we don't need such a huge dataset for this example."""

data = read_text('/ita.txt')
ita_eng = to_lines(data)
ita_eng = array(ita_eng)
ita_eng = ita_eng[:50000,:]

"""To ensure that our data was processed correctly, we will print out the array holding it."""

ita_eng

"""Now, since the text still contains unnecessary characters like punctuation, we will go ahead and filter all of that out using the string functions translate() and maketrans().

We'll also convert all the text to lowercase to make it easier for the model to train.
"""

# Remove punctuation
ita_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ita_eng[:,0]]
ita_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ita_eng[:,1]]

# Make all text lowercase
for i in range(len(ita_eng)):
    ita_eng[i,0] = ita_eng[i,0].lower()
    ita_eng[i,1] = ita_eng[i,1].lower()

"""# Converting Text to Sequences

Our next step is to convert our text into sequences that can be fed into our model. To do this, we have to first understand the distribution of sentence lengths for each language.

We'll start by creating two empty lists and then storing the lengths of sentences for each language in their respective lists.

To better visualize this, we'll use the DataFrame function from pandas to plot histograms of the sentence lengths.
"""

eng_l = []
ita_l = []

# populate the lists with sentence lengths
for i in ita_eng[:,0]:
      eng_l.append(len(i.split()))

for i in ita_eng[:,1]:
      ita_l.append(len(i.split()))

length_df = pd.DataFrame({'eng':eng_l, 'ita':ita_l})
length_df.hist(bins = 30)

"""**Tokenization**

Now we can begin converting our sentences into integer sequences by using Kera's Tokenizer() class. We'll go ahead and create tokenizers for both English and Italian and then display the number of tokens/words for both languages.

**Tokenizer Function:**
This function creates a tokenizer, fits the tokenizer on the input sequence, and then returns it. Fitting the tokenizer on the input sequence essentially means that the tokenizer is generating a unique integer for each word it sees in the input and creating a dictionary based on the integer:word matching it made.

**Prepping Tokenizers**
Here, we are creating a tokenizer for each language. Following that, we are getting the total vocab size which is the word_index + 1 because each word_index is the mapping dictionary which starts at 1 rather than 0. The 0 index is used as a reserve index which is used for padding in the case where sequences are not all the same length. Lastly, we are getting the maximum length for a sequence in both languages by using list comprehension.
"""

# function to build a tokenizer
def tokenization(lines):
      tokenizer = Tokenizer()
      tokenizer.fit_on_texts(lines)
      return tokenizer

# prepare english tokenizer
eng_tokenizer = tokenization(ita_eng[:, 0])
eng_vocab_size = len(eng_tokenizer.word_index) + 1
eng_length = max(len(line.split()) for line in ita_eng[:, 0])

print('English Vocabulary Size: %d' % eng_vocab_size)

# prepare Italian tokenizer
ita_tokenizer = tokenization(ita_eng[:, 1])
ita_vocab_size = len(ita_tokenizer.word_index) + 1
ita_length = max(len(line.split()) for line in ita_eng[:, 1])

print('Italian Vocabulary Size: %d' % ita_vocab_size)

"""By using the texts_to_sequences() function, we can convert the texts to sequences of integers based on the tokenizer mapping that was created. The last step is to then pad each sequence with 0's to ensure that each sequence is the same length. post padding just means that the 0's are being added to the end of each sequence."""

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
         # integer encode sequences
         seq = tokenizer.texts_to_sequences(lines)
         # pad sequences with 0 values
         seq = tf.keras.utils.pad_sequences(seq, maxlen=length, padding='post')
         return seq

"""# Building the Model

Because our data is currently all in one massive array, we need to split it up into training and testing sets. We'll use the train_test_split() function from scikit-learn to do so as it splits the text into two subsets of training and testing data. The test_size = 0.2 parameter specifies that we will use 20% of the data as the testing set and the rest as the training set. The random_state parameter is an optional parameter that we set to a random number like 12 which sets the seed for shuffling the data before splitting it. Having a set number will allow for more reproducibility in our program.
"""

# split data into train and test set
train,test= train_test_split(ita_eng,test_size=0.2,random_state= 12)

"""Now we can go ahead and encode both the training and testing datasets by using the encode_sequences() function we created earlier."""

# prepare training data
trainX = encode_sequences(ita_tokenizer, ita_length, train[:, 1])
trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])

# prepare validation data
testX = encode_sequences(ita_tokenizer, ita_length, test[:, 1])
#testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])

"""For our actual model, we will be using a sequential LSTM model with a total of 4 layers:

1. Embedding - The Embedding layer maps input sequences, which are represented as integers, into a vector space of dimension n. input_length represents the length of the input sequences; mask_zero = True tells the model to ignore values which are 0. The reason we use this layer is because reducing the dimensionality of the space makes it easier and faster for the model to learn meaningful patterns in the input data.
2. LSTM - The first LSTM layer is used to encode the input sequences into fixed-length vectors which hold information regarding the context and meaning of these sequences. It has n neurons.
3. RepeatVector - The RepeatVector layer is used to essentially repeat the LSTM layer's output out_len times to create the input sequence for the next LSTM layer. We need this because the output of the first LSTM layer may not be the same length as out_vocab which is what we want the model to spit out.
4. LSTM - The second LSTM layer takes in the output from the RepeatVector layer and then decodes the input sequence into an output sequence which contains vectors determine the most likely next word or token within the output sequence at a certain timestep. We use return_sequences=True to have the model return the full output sequence of intermediate timesteps instead of just a single vector containing the probability of the next word in the sequence since we want to generate a sequence of outputs.
2. Dense - This final layer is used to convert the output of the second LSTM to a probability distribution for the most likely words at each timestep of the output sequence. out_vocab corresponds to the size of the output vocabulary and the softmax activation will make sure that the probability distribution adds up to 1. It will select the most likely word at each timestep in the output sequence from the second LSTM layer.
"""

def build_model(in_vocab, out_vocab, in_len, out_len, n):
      model = Sequential()
      model.add(Embedding(in_vocab, n, input_length=in_len,
      mask_zero=True))
      model.add(LSTM(n))
      model.add(RepeatVector(out_len))
      model.add(LSTM(n, return_sequences=True))
      model.add(Dense(out_vocab, activation='softmax'))
      return model

"""For our model compilation, we will be using the Adam learning algorithm and the sparse_categorical_crossentropy loss function.


"""

# model compilation (with 512 hidden units)
model = build_model(ita_vocab_size, eng_vocab_size, ita_length, eng_length, 512)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

"""Now, to make sure we use the best model with the lowest validation loss, we will use checkpointing. The ModelCheckpoint() function saves the model weights as it trains. The monitor parameter will tell the program to check the validation loss as it trains. The verbose parameter tells the program to display messages while training. The save_best_only paramter tells the checkpoint to only save the weights that result in the best validation loss. Lastly, the mode parameter tells the model that our goal is to minimize the validation loss.


With all this completed, we can begin training the model with 20 epochs and a batch size of 128. The validation_split parameter allows us to specify what percent of the training set to use as validation. Using checkpoint as our callbacks will also tell the model to save checkpoints to checkpoint. These hyperparameters are not the most optimal and therefore can be modified to improve the model.
"""

filename = 'machine_translation_model'

# set checkpoint
checkpoint = ModelCheckpoint(filename, monitor='val_loss',
                             verbose=1, save_best_only=True,
                             mode='min')


# train model
history = model.fit(trainX, trainY,
                    epochs=20, batch_size=128, validation_split = 0.2,
                    callbacks=[checkpoint], verbose=1)

"""To visualize the training loss versus the validation loss, we will make a plot to display the two. As shown below, the validation loss stops decreasing at around 14 epochs."""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['train','validation'])

"""# Making Predictions

To make our predictions, we'll load our model from the checkpoint we made during training.
"""

model = load_model('machine_translation_model')

"""Since our model's predictions are still in integer form, we will need to create a function that decodes the integers. We will use a linear search function to find the words that match the integers in a sequence."""

def get_word(n, tokenizer):
      for word, index in tokenizer.word_index.items():
          if index == n:
              return word
      return None

"""Now that we have a function to map each integer to a word, we can create a function that will convert each full sequence and return the result as a string of words.

Each vector within our predictions holds the probabilities for each possible word in a sequence. By using argmax, we can grab the index for the highest probability word in each vector to predict the most accurate sequence.
"""

def predict_sequence(model, tokenizer, source):
    prediction = model.predict(source, verbose=0)[0]
    integers = [argmax(vector) for vector in prediction]
    target = list()
    for i in integers:
        word = get_word(i, tokenizer)
        if word is None:
            break
        target.append(word)
    return ' '.join(target)

"""Putting it all together, we will create one more function that will convert the first 30 integer sequences to text and then return the raw text, actual translation, and predicted translation.

The reason why we need to reshape the source in this function is because our predict_sequence function expects a 3D array for its prediction but the current source is a 2D array.
"""

# convert predictions into text (English)
def evaluate_model(model, tokenizer, sources, raw_dataset):
    actual, predicted, raw = list(), list(), list()
    for i, source in enumerate(sources):
    # translate encoded source text
        source = source.reshape((1, source.shape[0]))
        translation = predict_sequence(model, eng_tokenizer, source)
        raw_target, raw_src = raw_dataset[i]
        if i > 30:
          break
        actual.append(raw_target)
        predicted.append(translation)
        raw.append(raw_src)
    return actual, predicted, raw

"""We'll go ahead and call the function here"""

# evaluate model
a, p, r = evaluate_model(model, eng_tokenizer, testX, test)

"""To finish up the workshop, we will want to see how the model did so we'll create a table to display a samples of our predictions."""

pred_df = pd.DataFrame({'raw' : r[:30], 'actual' : a[:30], 'predicted' : p[:30]})

"""Printing samples of the predictions, we can see that our model did alright. Some sequences were translated correctly, however, others were a bit off. This can likely be improved through a larger amount of training data and tweaks to the hyperparameters during"""

# print 15 rows randomly
pred_df.sample(10)