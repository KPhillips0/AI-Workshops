# -*- coding: utf-8 -*-
"""NLP from Hugging Face - Kyle Phillips

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JVcoltOdCxWarp5F-FqwIRMtehcJmYx8
"""

!pip install transformers
!pip install sentencepiece

# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("translation", model="Helsinki-NLP/opus-mt-en-es")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-es")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-es")

pipe("my name is jeff")

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/bloomz-1b7")
model = AutoModelForCausalLM.from_pretrained("bigscience/bloomz-1b7")

tokenizer("nice to meet you")

tokenizer.decode([81, 1591, 427, 11890, 1152])

text = "Should I go to the store?"

tokens = tokenizer(text, return_tensors = 'pt')

tokens

o = model(tokens.input_ids)

o.logits[:, -1, :]

o.logits[:, -1, :].shape

next_probs = o.logits[0, -1, :]

next_probs

next_probs.argmax()

tokenizer.decode([3928])

tokenizer(" Positive")

tokenizer(" Negative")

review = "I'm lovin it"
prompt = "\"" + review + "\" Is the review positive or nagative?"

tokenization = tokenizer(prompt, return_tensors = 'pt')
o = model(**tokenization)

pos = o.logits[0,-1,139904]
neg = o.logits[0,-1,149414]
print("Positive", pos)
print("Negative", neg)